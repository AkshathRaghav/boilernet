from fastapi import FastAPI, File, UploadFile, HTTPException, Form, Query
from fastapi.responses import Response, JSONResponse
from fastapi import FastAPI, Request, Response
from pydantic import BaseModel
from PIL import Image
import threading
import io
import time
from queue import Queue
from typing import Dict, Optional, List
import base64

##########################
# Data Classes
##########################


class Task(BaseModel):
    task_id: int
    job_identifier: str  # Provided by front end; unique per session/job
    data: bytes  # Image file data
    filename: str  # For display purposes


class Event(BaseModel):
    event_id: int  # Allocated in the backend
    task_id: int  # References the front-end Task
    job_identifier: str  # The same job_identifier as the task
    command: str  # "store", "compute", "fetch", or "delete"
    status: str = "In Queue"  # "In Queue", "Processing", "Completed", "Failed"
    result: Optional[bytes] = None  # For computed/fetched images (empty for delete)


##########################
# Global Variables
##########################

# JOB_REGISTRY maps job_identifier to a dict with two keys:
#   "tasks": list of Task objects (from front end)
#   "events": list of Event objects (generated by operations)
job_registry: Dict[str, Dict[str, List]] = {}

# EVENT_QUEUE holds events waiting to be processed (active queue)
EVENT_QUEUE = Queue()

# COMPLETED_EVENT_STORE holds events that have finished processing.
# Keyed by event_id.
COMPLETED_EVENT_STORE: Dict[int, Event] = {}

# Counters for unique IDs
task_id_counter = 0
event_id_counter = 0

##########################
# Ethernet Worker
##########################


def ethernet_worker():
    """
    This worker continuously pulls events from EVENT_QUEUE and processes them.
    For events that operate on a task, it looks up the associated Task (from JOB_REGISTRY).
    For "store": it simply returns the original image.
    For "compute" and "fetch": it converts the image to grayscale.
    For "delete": it removes the task from the job.
    When done, the event status is set to "Completed" and it is moved to COMPLETED_EVENT_STORE.
    """
    global COMPLETED_EVENT_STORE
    while True:
        if not EVENT_QUEUE.empty():
            event = EVENT_QUEUE.get()
            event.status = "Processing"
            # Look up the associated task
            job = job_registry.get(event.job_identifier)
            if not job:
                event.status = "Failed"
            else:
                task = next(
                    (t for t in job["tasks"] if t.task_id == event.task_id), None
                )
                if not task:
                    event.status = "Failed"
                else:
                    try:
                        # For events that need image processing, open the image from task.data.
                        img = Image.open(io.BytesIO(task.data))
                        img_bytes = io.BytesIO()
                        if event.command in ["store", "fetch"]:
                            # Simply return the original image.
                            img.save(img_bytes, format="PNG")
                            event.result = img_bytes.getvalue()
                        elif event.command in ["compute"]:
                            # Convert to grayscale.
                            processed_img = img.convert("L")
                            processed_img.save(img_bytes, format="PNG")
                            event.result = img_bytes.getvalue()
                        elif event.command == "delete":
                            # For delete, we simply remove the task.
                            job["tasks"] = [
                                t for t in job["tasks"] if t.task_id != event.task_id
                            ]
                            event.result = b""  # No image to return.
                        event.status = "Completed"
                    except Exception as e:
                        event.status = "Failed"
            # Move event into COMPLETED_EVENT_STORE
            COMPLETED_EVENT_STORE[event.event_id] = event
        time.sleep(1)


threading.Thread(target=ethernet_worker, daemon=True).start()

##########################
# FastAPI Application
##########################

app = FastAPI()

##########################
# Helper Functions
##########################


def get_existing_event(
    job_identifier: str, task_id: int, command: str
) -> Optional[Event]:
    """
    Check JOB_REGISTRY for an existing event with the same command and task_id.
    Return it if found; otherwise, return None.
    """
    job = job_registry.get(job_identifier)
    if not job:
        return None
    for ev in job["events"]:
        if ev.task_id == task_id and ev.command == command:
            return ev
    return None


##########################
# Endpoints
##########################

@app.middleware("http")
async def json_timing_middleware(request: Request, call_next):
    start = time.perf_counter()

    # 1) Read request body (so we can see the JSON)
    body_bytes = await request.body()
    try:
        req_json = json.loads(body_bytes.decode("utf-8"))
    except Exception:
        req_json = None

    # 2) Call the actual endpoint
    response = await call_next(request)

    # 3) Read response body if it's JSON
    resp_json = None
    if response.media_type == "application/json":
        # pull in the full body
        body = b""
        async for chunk in response.body_iterator:
            body += chunk
        try:
            resp_json = json.loads(body.decode("utf-8"))
        except Exception:
            resp_json = None
        # restore body for FastAPI to send it
        response.body_iterator = iter([body])

    end = time.perf_counter()
    elapsed_ms = (end - start) * 1000

    # 4) Record stats
    print({
        "path": request.url.path,
        "method": request.method,
        "request_json": req_json,
        "response_json": resp_json,
        "time_ms": round(elapsed_ms, 2)
    })

    return response


# Create Job Endpoint: Receives a file from the front end, creates a Task, and (if not already present)
# creates a "store" event to put the image into EVENT_QUEUE.
@app.post("/job_compute")
async def create_job(
    image: UploadFile = File(...),
    operation: str = Form(...),  # Expecting "store" here
    info: str = Form(""),
    job_identifier: str = Form(...),
    task_index: int = Form(0),
):
    global task_id_counter, event_id_counter
    # Create a new Task from the uploaded file.
    task_id_counter += 1
    img_bytes = await image.read()
    new_task = Task(
        task_id=task_id_counter,
        job_identifier=job_identifier,
        data=img_bytes,
        filename=image.filename,
    )
    # Initialize job entry if needed.
    if job_identifier not in job_registry:
        job_registry[job_identifier] = {"tasks": [], "events": []}
    job_registry[job_identifier]["tasks"].append(new_task)
    # Before adding a new event, check if a "store" event for this task already exists.
    existing = get_existing_event(job_identifier, new_task.task_id, "store")
    if not existing:
        event_id_counter += 1
        new_event = Event(
            event_id=event_id_counter,
            task_id=new_task.task_id,
            job_identifier=job_identifier,
            command="store",
        )
        job_registry[job_identifier]["events"].append(new_event)
        EVENT_QUEUE.put(new_event)
    return {"message": "Task added for processing", "task_id": new_task.task_id}


# Compute Operation Endpoint:
# For a given job_identifier and task_id, check if a "compute" event exists.
# If not, create one and enqueue it. Then, if the event is completed (in COMPLETED_EVENT_STORE),
# return the computed image (grayscale); if not, return its status.
@app.get("/job_compute/{job_identifier}")
def compute_operation(job_identifier: str, task_id: int = Query(...)):
    global event_id_counter
    if job_identifier not in job_registry:
        raise HTTPException(status_code=404, detail="Job not found")
    # Check if the task exists.
    task = next(
        (t for t in job_registry[job_identifier]["tasks"] if t.task_id == task_id), None
    )
    if not task:
        raise HTTPException(status_code=404, detail="Task not found in job")
    # Check for an existing compute event.
    existing = get_existing_event(job_identifier, task_id, "compute")
    if not existing:
        event_id_counter += 1
        new_event = Event(
            event_id=event_id_counter,
            task_id=task_id,
            job_identifier=job_identifier,
            command="compute",
        )
        job_registry[job_identifier]["events"].append(new_event)
        # Before enqueueing, check if an event with same command and task_id is already in EVENT_QUEUE.
        # (Our helper already ensures we donâ€™t create duplicates.)
        EVENT_QUEUE.put(new_event)
        event_to_use = new_event
    else:
        event_to_use = existing
    # Check if this event is completed.
    if event_to_use.event_id in COMPLETED_EVENT_STORE:
        if event_to_use.result:
            encoded = base64.b64encode(event_to_use.result).decode("utf-8")
        else:
            encoded = None
        return {
            "job_identifier": job_identifier,
            "command": "compute",
            "task_id": task_id,
            "status": "COMPLETE",
            "image": encoded,
        }
    else:
        return {
            "job_identifier": job_identifier,
            "command": "compute",
            "task_id": task_id,
            "status": event_to_use.status,
        }


# Fetch Operation Endpoint:
# Similar to compute, but for the "fetch" command.
@app.get("/job_fetch/{job_identifier}")
def fetch_operation(job_identifier: str, task_id: int = Query(...)):
    global event_id_counter
    if job_identifier not in job_registry:
        raise HTTPException(status_code=404, detail="Job not found")
    task = next(
        (t for t in job_registry[job_identifier]["tasks"] if t.task_id == task_id), None
    )
    if not task:
        raise HTTPException(status_code=404, detail="Task not found in job")
    existing = get_existing_event(job_identifier, task_id, "fetch")
    if not existing:
        event_id_counter += 1
        new_event = Event(
            event_id=event_id_counter,
            task_id=task_id,
            job_identifier=job_identifier,
            command="fetch",
        )
        job_registry[job_identifier]["events"].append(new_event)
        EVENT_QUEUE.put(new_event)
        event_to_use = new_event
    else:
        event_to_use = existing
    if event_to_use.event_id in COMPLETED_EVENT_STORE:
        if event_to_use.result:
            encoded = base64.b64encode(event_to_use.result).decode("utf-8")
        else:
            encoded = None
        return {
            "job_identifier": job_identifier,
            "command": "fetch",
            "task_id": task_id,
            "status": "COMPLETE",
            "image": encoded,
        }
    else:
        return {
            "job_identifier": job_identifier,
            "command": "fetch",
            "task_id": task_id,
            "status": event_to_use.status,
        }


# Delete Operation Endpoint:
# For delete, once the event is complete, simply return the task_id being deleted.
@app.get("/job_delete/{job_identifier}")
def delete_operation(job_identifier: str, task_id: int = Query(...)):
    global event_id_counter
    if job_identifier not in job_registry:
        raise HTTPException(status_code=404, detail="Job not found")
    task = next(
        (t for t in job_registry[job_identifier]["tasks"] if t.task_id == task_id), None
    )
    if not task:
        raise HTTPException(status_code=404, detail="Task not found in job")
    existing = get_existing_event(job_identifier, task_id, "delete")
    if not existing:
        event_id_counter += 1
        new_event = Event(
            event_id=event_id_counter,
            task_id=task_id,
            job_identifier=job_identifier,
            command="delete",
        )
        job_registry[job_identifier]["events"].append(new_event)
        EVENT_QUEUE.put(new_event)
        event_to_use = new_event
    else:
        event_to_use = existing
    if event_to_use.event_id in COMPLETED_EVENT_STORE:
        # For delete, we just return the task_id being deleted.
        # Also remove the event from the job's events list and from COMPLETED_EVENT_STORE.
        job_registry[job_identifier]["events"] = [
            ev
            for ev in job_registry[job_identifier]["events"]
            if ev.event_id != event_to_use.event_id
        ]
        COMPLETED_EVENT_STORE.pop(event_to_use.event_id, None)
        return {
            "job_identifier": job_identifier,
            "command": "delete",
            "task_id": task_id,
            "status": "COMPLETE",
            "deleted": task_id,
        }
    else:
        return {
            "job_identifier": job_identifier,
            "command": "delete",
            "task_id": task_id,
            "status": event_to_use.status,
        }


@app.delete("/job/{job_identifier}")
def delete_job(job_identifier: str):
    """
    Immediately remove the entire job (all tasks and events) that are not currently processing.
    If no tasks remain after removal, the job is deleted from JOB_REGISTRY.
    """
    if job_identifier not in job_registry:
        raise HTTPException(status_code=404, detail="Job not found")
    job = job_registry[job_identifier]
    # Remove events and tasks that are not in processing.
    removable_tasks = [
        t
        for t in job["tasks"]
        if not any(
            ev
            for ev in job["events"]
            if ev.task_id == t.task_id and ev.status == "Processing"
        )
    ]
    removable_events = [ev for ev in job["events"] if ev.status != "Processing"]
    job["tasks"] = [t for t in job["tasks"] if t not in removable_tasks]
    job["events"] = [ev for ev in job["events"] if ev not in removable_events]
    if not job["tasks"]:
        job_registry.pop(job_identifier)
        return {"job_identifier": job_identifier, "status": "DELETED"}
    else:
        processing_ids = [
            t.task_id
            for t in job["tasks"]
            if any(
                ev
                for ev in job["events"]
                if ev.task_id == t.task_id and ev.status == "Processing"
            )
        ]
        return {
            "job_identifier": job_identifier,
            "status": "PARTIALLY DELETED",
            "remaining_processing": processing_ids,
        }
